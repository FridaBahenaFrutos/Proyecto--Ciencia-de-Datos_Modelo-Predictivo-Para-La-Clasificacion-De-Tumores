# -*- coding: utf-8 -*-
"""Modelo Predictorio para la clasificacion de tumores_Frida Bahena Frutos .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y82hgy_lMqmMeszjCun5m4lf6thynNwU

#Proyecto Final
## Introduccion a la ciencia de datos
##**Predictor de estado tumoral benigno o maligno en cancer de mamá**

**Objetivo**: Predecir si el cáncer es benigno o maligno.\
Desarrollar un modelo predictivo basado en técnicas de ciencia de datos que permita clasificar de manera precisa y eficiente los tumores de cáncer de mama como benignos o malignos, utilizando un conjunto de datos con características detalladas de los tumores,contribuyendo así a la detección temprana y al tratamiento eficaz del cáncer de mama. Los datos se recuperaron de https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data.
"""

#librerias
import pandas as  pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pandas as pd
from sklearn.metrics import classification_report
from sklearn import tree

#leemos el data y lo visualizamos con pandas
data = pd.read_csv("data.csv")

"""#**Visualizamos datos (EDA)**"""

data.head(5)
#M=maligno, B=benigno

len(data.columns)

for i in data:
  print('datos unicos en:',i, len(data[i].unique()))

#observamos el dataframe
data.info()

data.describe()

#definicion de las variables numericas y categoricas del DataFrame
varibles_numericas1 = data.select_dtypes(include = [np.number])
variables_categoricas2 = data.select_dtypes(exclude= [np.number])

#se viasualizan los histogramas de cada columna
for i in varibles_numericas1:
  plt.title('Histograma:'+i)
  plt.hist(data[i])
  plt.grid()
  plt.show()

#se visualizan los diagramas de cajas para cada columna
for i in varibles_numericas1:
  plt.title('box:'+i)
  plt.boxplot(data[i])
  plt.grid()
  plt.show()

"""#**DATA2 CLEANING**


"""

#se buscan valores faltantes
val_null = data.isnull().sum()
print(val_null)

#eliminamos la columna con ceros que era justamente la ultima
data2 = data.dropna(axis = 1)

#veamos si hay valores duplicados
data2.duplicated()
#en este caso no hay filas duplicadas

#visualizacion del DataFrame2 sin la ultima columna
data2

#definicion de la variables categoricas y numericas (data2) para su uso en el procesamiento de datos
varibles_numericas = data2.select_dtypes(include = [np.number])
variables_categoricas = data2.select_dtypes(exclude= [np.number])

#definir solo las columnas numericas
columnas_numericas= (data2.columns).drop('diagnosis')

columnas_numericas

def tratar_outliers(df, columna):
    """
    Sustituye los outliers en una columna específica por los percentiles 25 o 75.
    Un valor se considera outlier si está fuera de 1.5 * IQR.
    """
    Q1 = df[columna].quantile(0.25)
    Q3 = df[columna].quantile(0.75)
    IQR = Q3 - Q1
    limite_inferior = Q1 - 1.5 * IQR
    limite_superior = Q3 + 1.5 * IQR
    df[columna] = df[columna].clip(lower=limite_inferior, upper=limite_superior)
    return df

for i in columnas_numericas:
  tratar_outliers(data2, i)

data2

#solo para visualizacion del tratamiento de valores atipicos
for i in columnas_numericas:
  plt.title('box:'+i)
  plt.boxplot(data2[i])
  plt.grid()
  plt.show()

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
def estandarizar_datos(data, columnas_numericas):

    """
    Estandariza las columnas numéricas del DataFrame para que tengan media 0 y desviación estándar 1.
    """
    escalador = StandardScaler()
    data[columnas_numericas] = escalador.fit_transform(data[columnas_numericas])
    return data

#para un manejo mas facil, se realiza una normalizacion de los datos
estandarizar_datos(data2, columnas_numericas)

varibles_numericas.corr()

corr_matrix = varibles_numericas.corr()
sns.set(rc = {'figure.figsize':(10,6)})
sns.heatmap(corr_matrix,annot=False,fmt=".01"

# Eliminación de redundancia columnar
def eliminar_redundancia_columnar(data, umbral_correlacion=0.9):
    """
    Elimina columnas altamente correlacionadas para evitar la multicolinealidad.
    """
    matriz_corr = data.corr().abs()
    triangulo_superior = matriz_corr.where(np.triu(np.ones(matriz_corr.shape), k=1).astype(bool))
    columnas_a_eliminar = [columna for columna in triangulo_superior.columns if any(triangulo_superior[columna] > umbral_correlacion)]
    data = data.drop(columnas_a_eliminar, axis=1)
    return data

data3 = data2.drop(['diagnosis'], axis=1)

eliminar_redundancia_columnar(data3, 0.9)

#para ver la cardinalidad de cada columna, como todas nuestras columnas son numricas no hace falta aplicar ningun metodo
#de reduccion de cardinalidad. la columna 'diagnosis' es la unica categorica y se usara para el arbol
for i in data3:
  print('numero de variables de la columna: \t', i, len(data3['area_mean']))

data3

len(data3.columns)

"""#**Modelado de datos - Random Forest**"""

#usando rando tree comenzamos por dividimos por etiquetas los datos
data3.columns

#dividir en x y y
x = data3[['id', 'radius_mean', 'texture_mean', 'perimeter_mean',
       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',
       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',
       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
       'fractal_dimension_se', 'radius_worst', 'texture_worst',
       'perimeter_worst', 'area_worst', 'smoothness_worst',
       'compactness_worst', 'concavity_worst', 'concave points_worst',
       'symmetry_worst', 'fractal_dimension_worst']]

y = data['diagnosis']

# Dividir el conjunto de datos en entrenamiento y prueba
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)



#Árbol de decisión
ad=tree.DecisionTreeClassifier(max_features=3)

#Bosque aleatorio
rf = RandomForestClassifier(n_estimators=30  , random_state=0)


# Entrenar el modelo
rf=rf.fit(x_train, y_train)
ad=ad.fit(x_train, y_train)

# Predecir las etiquetas de los datos de prueba
y_pred_rf = rf.predict(x_test)
y_pred_ad = ad.predict(x_test)

# Calcular la precisión del modelo
accuracy_rf = accuracy_score(y_test, y_pred_rf)
accuracy_ad = accuracy_score(y_test, y_pred_ad)
print(f'Precisión del modelo Random Forest: {accuracy_rf:.2f}')
print(f'Precisión del modelo Desicion Tree: {accuracy_ad:.2f}')

"""#**Resultados**"""

tree.plot_tree(ad)

"""#**Evaluación**"""

#evaluacion de la precision de la prediccion
report_rf = classification_report(y_test, y_pred_rf)
print(report_rf)

report_ad = classification_report(y_test, y_pred_ad)
print(report_ad)